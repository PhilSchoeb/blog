---
layout: post
title: Milestone 2
---

## Suivi des expériences

Nos modèles et leurs performances associées peuvent être trouvés [ici](https://wandb.ai/IFT6758-2024-A05/projects)

## Ingénierie des caractéristiques

En utilisant notre fonction load_events_dataframe de la milestone 1, nous pouvons charger les données pour les saisons qui nous intéressent.

### Question 1 :

#### Angle et distance par rapport au filet

Les caractéristiques de distance et d'angle par rapport au filet avaient déjà été ajoutées lors du premier jalon.
Nous avons tout de même ajusté la caractéristique `goal_angle` pour qu'elle soit comprise entre -180 et 180 degrés,
ce qui n'était pas le cas dans la version précédente.
Les valeurs négatives correspondent à un angle à gauche du point de vue du gardien de but, tandis que les valeurs 
positives correspondent à un angle à droite du point de vue du gardien de but.
La valeur 0 correspond à un tir en face du gardien de but, tandis que les valeurs plus grandes que 90 ou plus petites 
que -90 correspondent à un angle derrière le gardien de but. Ceci peut avoir lieu lors d'un rebond.

Voici un schéma illustrant l'angle du tir par rapport au filet :

<img class="figure" src="/images/player_to_goal_2.png" />

#### Filet vide

Nous avons ajouté la caractéristique `is_empty_net` pour indiquer si le filet était vide lors du tir.
Pour ce faire, nous avons utilisé les données `situationCode`, `eventOwnerTeamId` et `homeTeam` pour déterminer si le
gardien de but adverse était présent ou non dans le filet lors du tir.

#### Cible de l'apprentissage supervisé

Nous avons ajouté la caractéristique `is_goal` à partir de la colonne `event_type` pour indiquer si un tir a abouti à un but.
Cette caractéristique booléenne servira de cible pour l'apprentissage supervisé.

#### Figures

<img src="/images/feature_engineering_1/distance_distribution.png" />

Si on s'intéresse à la distribution des tirs en fonction de la distance, on observe que la majorité des tirs sont faits à moins de 60 pieds du but.

<img src="/images/feature_engineering_1/angle_distribution.png" />

Pour ce qui est de l'angle du tir par rapport au but, on voit que la plupart des buts sont réalisés juste en face du filet avec des angles proches de 0. On voit aussi une certaine symétrie des données, où il y a pratiquement autant de buts et de tirs des deux côtés du filet.

<img src="/images/feature_engineering_1/joint_plot.png" />

Enfin, le joint plot nous montre que plus, on est loin du filet, plus l'angle de tir par rapport au filet est assez réduit. À l'inverse, plus on s'approche du filet, plus on observe que les tirs ont tendance à avoir des angles bien plus grands, voire très grands pour certains, notamment puisqu'on va retrouver ici des tirs qui se font derrière le filet.

#### Données aberrantes : problèmes de coordonnées

Nous avons observé des données aberrantes dans les coordonnées des tirs.
Certains événements ont des valeurs  `xCoord` et `zoneCode` incompatibles.
Cela pourrait entraîner une mauvaise détermination du côté de l'équipe à domicile, ce qui fausserait les distances et les angles.

Nous avons ajouté un test dans le notebook pour vérifier si la détermination du côté est correcte.
Nous pouvons vérifier lorsque l'événement est en zone Offensive ou Défensive :

- Si le côté est **Offensif** et que les `xCoord` et `xGoal` n'ont pas le même signe, il y a un problème.
- Si le côté est **Défensif** et que les `xCoord` et `xGoal` ont le même signe, il y a un problème.

Il y avait environ **2500** événements sur 647679 avec ce problème.

Pour trouver le bon côté de l'équipe à domicile, nous nous appuyons maintenant sur plusieurs événements (65 événements).
Pour déterminer le côté de l'équipe à domicile, nous faisons un vote pour le côté le plus fréquent.
Cela réduit le nombre d'événements problématiques à **550**.

Nous ne pouvons pas corriger les événements restants, car nous ne pouvons pas déterminer lequel du code de zone ou des coordonnées est incorrect.

### Question 2 :

Si maintenant, on s'intéresse uniquement au taux de but et sa relation à la distance, on observe qu'entre 0 et 60 pieds depuis le filet, la relation est plutôt linéaire, avec un plus haut succès plus, on se rapproche du filet. Cependant, à partir d'au-delà de 60 pieds, la relation est plus uniforme, et pour de grandes distances, on se retrouve à autour d'environ 9% de succès.

<img src="/images/feature_engineering_1/distance_success_rate.png" />

Pour ce qui est de l'angle de tir par rapport au milieu du filet (qui représente 0 degré), on voit qu'on a tendance à avoir une relation plutôt linéaire, si on prend la valeur absolue de l'angle et qu'on néglige de quel côté se fait le tir, jusqu'à 90 degrés. Effectivement, plus on s'éloigne de 0 degrés plus le taux de succès baisse. Cependant au-delà d'un angle de 90 degrés (donc quand on passe derrière le filet), la tendance s'inverse. On a également quelques données de tirs qui se font à des angles très extrêmes avec des taux de succès de 100%. Cela pourrait représenter de très rares tirs sur l'ensemble de données, d'ailleurs au-delà de 90 degrés d'angle, on ne parle que des 0.6% de notre ensemble de données. Ces données sont peut-être donc à négliger, ou pourraient être des données aberrantes. 

<img src="/images/feature_engineering_1/angle_success_rate.png" />

### Question 3 :

Si on s'intéresse à la relation entre distance, s'il y a un goal dans le filet ou non et le taux de buts associé, les données semblent globalement logiques cependant certaines observations pourraient laisser penser qu'il y a des données aberrantes. Le taux de buts est toujours plus haut quand le goal est absent, sauf pour des distances entre 80 et 100 pieds. Le taux de buts reste relativement bas à mesure qu'on s'éloigne du filet quand le goal est présent dans le filet (en-dessous de 10%). On pourrait considérer quel cela reste haut, cependant, on ne prend pas ici en compte le nombre de tirs, et on sait que l'énorme majorité des tirs se font à moins de 60 pieds du filet comme on l'a vu au début. Donc ces événements restent très rares in fine. Les hauts taux de buts à une grande distance du filet quand le goal est absent correspondent probablement aux moments où le goal quitte le filet en fin de partie et où des tirs d'une grande distance peuvent être produits. En conclusion, les données ne semblent pas trop aberrantes ici. 

<img src="/images/feature_engineering_1/distance_success_rate_net_status.png" />

Cependant, il est important de rappeler que dans la milestone 1 nous avions utilisé des méthodes pour nous débarrasser au préalable de certaines données aberrantes (comme décrit dans la question 1 ci-dessus).

## Modèles de base

En exécutant le fichier `simple_models.py` tel que `python -m simple_models.py`, on peut séparer nos données en ensemble d'entraînement et de validation et sauver les résultats sur wandb tout en produisant des graphiques mêlant les métriques tous nos modèles simples ensemble.

### Question 1 :

Nous avons produit nos divisions d'entraînement et de validations en chargeant les données des saisons 2016 à 2019, puis en séparant aléatoirement 20% des données pour l'ensemble de validation. 

Pour le modèle de classification logistique avec la distance par rapport au goal et les paramètres par défaut, on obtient une précision de 0.9039698177763522. Cependant, si on regarde les prédictions faites en terme des étiquettes attribuées, comme montré sur la matrice de confusion ci-dessous, on observe que le modèle n'a pas été capable de prédire un seul but. Il estime que tous tirs sont des non-buts. 

<img src="/images/simple_models/confusion_matrix.png" />

Cela nous indique que la précision en tant que tel n'est pas une métrique suffisante pour évaluer la qualité de notre classifieur logistique. Effectivement, ce chiffre de 90% correspond en fait à la quantité de non-buts sur notre ensemble de tirs. Le modèle est incapable de classifier les buts comme buts, mais classifie tout comme non-buts. Sauf que 90% des données sont des non-buts.

### Question 2 :

Pour les visualisations, le fichier graphs.py offre plusieurs fonctions, notamment une qui crée les quatre visualisations (four_graphs, intéressant pour les sauver sur l'entrée wandb associée avec le modèle), et une autre qui combine qui permet d'avoir des visualisations multi-modèles (four_graphs_multiple_models).

### Question 3 :

Les résultats pour nos 3 modèles simples en plus d'une ligne de base aléatoire sont les suivants :

<img src="/images/simple_models/roc_curve.png" />

La courbe ROC nous indique que les modèles basés sur la distance et celui combinant distance et angle ont le mieux fonctionné, avec un AUC de 0.70. Le modèle basé sur l'angle peine à faire mieux que le modèle aléatoire.

<img src="/images/simple_models/goal_rate.png" />

<img src="/images/simple_models/cumul_goal_prop.png" />

Le graphique du taux de buts sur les centiles des probabilités nous confirme un peu plus cela, et nous indique également que nos deux "meilleurs" modèles simples produisent des résultats similaires. De même pour celui du nombre de buts cumulé.

<img src="/images/simple_models/calibration_curve.png" />

Enfin la courbe de calibration nous montre comment performent nos modèles. On observe que les principales limitations de nos modèles ici sont qu'ils ne sont pas capables de prédire une grande variabilité de probabilités. Les modèles les plus performants ici sont capables de prédire correctement des probabilités de taux de buts jusqu'à 20%, qui correspondent aux taux de buts réellement observés. 

Ainsi, on a ici des modèles qui nous montrent que la distance peut être intéressante pour prédire les taux de buts, mais sa puissance en tant que feature est assez limitée puisqu'elle prédit seulement une fraction des probabilités possibles.  

### Question 4 :

Pour chaque entrée Wandb, on a : la précision, les métriques de performances (graphiques) et un fichier joblib en artifact pour télécharger le modèle.

Classifieur logistique — [distance](https://wandb.ai/IFT6758-2024-A05/simple_model_logistic_regression/runs/7772lzd5)

Classifieur logistique — [angle](https://wandb.ai/IFT6758-2024-A05/simple_model_logistic_regression/runs/ture99jg)

Classifieur logistique — [distance et angle](https://wandb.ai/IFT6758-2024-A05/simple_model_logistic_regression/runs/mll0tete)

## Ingénierie des caractéristiques II
### Liste des caractéristiques créées

| **Nom de la colonne**               | **Description lisible par un humain**                                                                                             |
|-------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|
| **time_in_period_seconds**          | Temps écoulé dans la période actuelle, converti en secondes à partir du format `MM:SS`.                                           |
| **game_seconds**                    | Nombre total de secondes écoulées dans le jeu, calculé comme : `(period_number - 1) * 1200 + time_in_period_seconds`.             |
| **last_event_type**                 | Type de l’événement précédent (par exemple, `shot-on-goal`, `blocked-shot`, `goal`, etc.).                                        |
| **last_x**                          | Coordonnée X de l’événement précédent (sur le terrain).                                                                           |
| **last_y**                          | Coordonnée Y de l’événement précédent (sur le terrain).                                                                           |
| **time_since_last_event**           | Temps écoulé (en secondes) depuis l’événement précédent.                                                                          |
| **distance_from_last_event**        | Distance entre les coordonnées (X, Y) de l’événement précédent et celles de l’événement actuel (en unités du terrain).            |
| **is_rebound**                      | Indicateur booléen : `True` si l’événement précédent était un tir (`shot-on-goal`, `missed-shot`, `blocked-shot`), sinon `False`. |
| **speed**                           | Vitesse calculée comme : `distance_from_last_event / time_since_last_event`. Si le temps écoulé est 0, vitesse = 0.               |
| **last_angle**                      | Angle du tir précédent (en degrés) par rapport au filet.                                                                          |
| **absolute_angle_change**           | Changement d’angle absolu entre le tir actuel et le tir précédent, calculé comme : `abs(goal_angle) + abs(last_angle)`.           |
| **power_play_time_elapsed (BONUS)** | Temps écoulé (en secondes) depuis le début de l’avantage numérique (jeu de puissance). Réinitialisé à 0 en fin de pénalité.       |
| **shooting_team_skaters (BONUS)**   | Nombre de patineurs sur glace pour l'équipe effectuant le tir.                                                                    |
| **opposing_team_skaters (BONUS)**   | Nombre de patineurs sur glace pour l'équipe adverse.                                                                              |


---

Lien vers l'expérience qui stocke l'artefact DataFrame filtré pour le jeu spécifié [ici](https://wandb.ai/IFT6758-2024-A05/ift6758-milestone-2/artifacts/dataset/wpg_v_wsh_2017021065/v1/files).

## Modèles avancés

### Question 1 :

Voici les 4 figures demandées :

<img src="/images/roc_basic_xgboost.png" alt="ROC Curve" />
<img src="/images/goal_rate_vs_proba_basic_xgboost.png" alt="Goal Rate vs Probability" />
<img src="/images/cumul_goal_prop_vs_proba_basic_xgboost.png" alt="Cumulative Goal Propensity vs Probability" />
<img src="/images/calibration_curve_basic_xgboost.png" alt="Calibration Curve" />

La configuration de l'ensemble d'entrainement et de validation s'est faite de même manière qu'à la partie 3 (modèles simples). C'est-à-dire une séparation 80-20 aléatoire (même seed pour les deux parties).

En comparant ces figures à celles obtenues à la question 3, on se rend compte que la performance du modèle XGBoost surpasse celle de la régression logistique. En effet, dans le graphique de la courbe ROC, on voit que l'AUC est maintenant supérieure. La figure du taux de buts en fonction du centile de la probabilité prédite est très semblable entre XGBoost et les meilleurs modèles de la partie 3. La seule différence est que la courbe du XGBoost semble être plus stable (moins de variations non justifiées). Les deux figures trois sont très similaires, si l'on oublie les moins bons modèles de régression logistique. Finalement, on voit la plus grande différence dans la 4e figure. Le modèle XGBoost est beaucoup mieux calibré que les autres.

Voici le lien vers l'expérience Wandb [ici](https://wandb.ai/IFT6758-2024-A05/ift6758-milestone-2/runs/nmzfzlu7).

### Question 2 :

Premièrement, voici les hyper-paramètres que nous avons optimisés ainsi que leurs différentes options testées :

- Booster = "gbtree" ou "dart"
- eta = 0.1, 0.3, 0.5 ou 0.8
- gamma = 0, 10 ou 100
- max_depth = 4, 6, 8 ou 12

Notre méthode d'optimisation a été d'effectuer une recherche par grille avec une validation non croisée. Nous avons fait ce choix pour pouvoir tester toutes les 96 configurations possibles sans que le temps de computation soit trop élevé. Il est important de noter que nous avons tenté d'optimiser l'AUC (area under curve) par la sélection des hyper-paramètres, car nous nous intéressons uniquement aux probabilités qu'il y ait un but.

Regardons maintenant l'influence de chaque hyper-paramètre, en supposant faussement qu'ils sont indépendants les uns par rapport aux autres, sur l'accuracy et l'AUC moyenne :

<img src="/images/booster.png" alt="Booster config" />
<img src="/images/eta.png" alt="eta config" />
<img src="/images/gamma.png" alt="gamma config" />
<img src="/images/max_depth.png" alt="max_depth config" />

On peut voir que le choix de booster ne semble pas avoir d'effet, mais les autres hyper-paramètres peuvent être optimisés. En prenant la configuration qui maximise l'AUC, nous avons sélectionné la configuration suivante : 

Booster = "gbtree", eta = 0.3, gamma = 0 et max_depth = 8. 

Voici une visualisation pour s'en convaincre :

<img src="/images/roc_curve_hp.png" alt="Roc curve for all different hyperparameters configuration" />

Voici maintenant les quatre figures demandées pour notre modèle sélectionné :

<img src="/images/roc_best_xgboost_gbtree_0.3_0_8.png" alt="ROC Curve" />
<img src="/images/goal_rate_vs_proba_best_xgboost_gbtree_0.3_0_8.png" alt="Goal Rate vs Probability" />
<img src="/images/cumul_goal_prop_vs_proba_best_xgboost_gbtree_0.3_0_8.png" alt="Cumulative Goal Propensity vs Probability" />
<img src="/images/calibration_curve_best_xgboost_gbtree_0.3_0_8.png" alt="Calibration Curve" />

En comparant ces figures à celles obtenues à la partie 5.1, on voit que la courbe ROC a la même forme, mais son aire sous la courbe est supérieure maintenant. Par rapport, à la figure du goal_rate en fonction du centile de la probabilité prédite, on constate qu'une augmentation du centile a maintenant plus de chance qu'avant d'augmenter le taux de but et aussi que le taux de but augmente quasi exponentiellement à partir du 80e centile. On observe aussi que la valeur maximale de goal_rate atteinte est deux fois plus grande. Passons à la figure du pourcentage cumulatif de buts en fonction du centile de la probabilité prédite. Ici, on voit que la courbe est plus prononcée et cela implique qu'il y a beaucoup moins de buts parmi les points prédits à probabilité basse et inversement, il y a plus de points prédits à haute probabilité qui sont des buts. Finalement, on observe la courbe de calibration et on voit directement que les probabilités prédites sont plus élevées. Nos deux courbes suivent assez bien la droite optimale et les deux prédisent des probabilités un peu trop basses pour les plus grandes probabilités prédites.

Avant de conclure sur cette question, juste mentionner que nous avons réalisé que l'hyper-paramètre num_round (pour nombre de rounds) représente le nombre d'arbres "boosted" utilisés. Cela fait de num_round le principal hyper-paramètre de XGBoost et pourtant nous ne l'avions pas optimisé, donc nous l'avons évalué de manière indépendante, par manque de temps, à partir de l'accuracy et l'AUC pour notre configuration choisie avec différents num_rounds :

num_rounds = 2, 5, 10, 15, 20 et 30

Voici le résultat obtenu :

<img src="/images/num_rounds.png" alt="num_rounds" />

Assez ironiquement, on voit que la valeur par défaut (10) est la meilleure donc nous sélectionnons pour notre configuration finale num_rounds = 10.

Pour finir, voici le lien de l'expérience sur Wandb [ici](https://wandb.ai/IFT6758-2024-A05/best%20xgboost). 

### Question 3 :

Parmi les caractéristiques à notre disposition, nous en avons préalablement retiré un bon nombre parce qu'elles étaient fortement corrélées voire quasi-identiques. Voici donc les caractéristiques de base pour notre modèle XGBoost :

- period_number

- period_type

- is_empty_net

- x_coord

- y_coord

- shot_type

- goal_distance

- goal_angle

- time_in_period_seconds

- game_seconds

- last_event_type

- last_x

- last_y

- time_since_last_event

- distance_from_last_event

- is_rebound

- speed

- last_angle

- absolute_angle_change

- power_play_time_elapsed

- shooting_team_skaters

- opposing_team_skaters

Notre première méthode utilisée pour la sélection de caractéristiques a été la sélection par variance. Cette dernière consiste à retirer les caractéristiques ayant une variance trop faible (sous un seuil). Pour l'implémenter nous avons utilisé la fonction VarianceThreshold de sklearn.feature_selection. Nous avons testé notre meilleur modèle XGBoost sur l'ensemble des caractéristiques sélectionnées par deux itérations différentes de la sélection. Ces dernières utilisaient, respectivement, un seuil de 0.160 et 0.227 et sélectionnaient un total de 20 et 19 caractéristiques.

Par la suite, nous avons utilisé un méthode de sélection univariée qui se base sur des tests statistiques. Cette dernière est SelectKBest de sklearn.feature_selection qui ne garde que les k meilleures caractéristiques selon le test. Nous avons testé cette méthode de la même manière que la première mais avec 4 valeurs différentes de k. Elles sont 5, 10, 15 et 20 (considérant que nous commençons avec 22 caractéristiques). 

Pour comparer ces différentes méthodes nous nous sommes appuyés sur l'AUC pour les mêmes raisons qu'en 5.2. Voici donc une figure qui permet de comparer les méthodes de sélection :

<img src="/images/feature_selection.png" alt="feat_sel" />

On constate alors que le meilleur ensemble de caractéristiques est l'original. Cela démontre que le modèle XGBoost utilise toutes les caractéristiques à sa disposition. En effet, on voit une grande corrélation entre l'AUC et le nombre de caractéristiques du modèle. On réalise maintenant qu'il aurait été plus intéressant de faire exprès de donner des caractéristiques redondantes à notre modèle de base au lieu de le pré-filtrer afin de voir l'effet positif de la sélection de caractéristiques.

De ce fait, les courbes demandées sont celles présentées dans la section 5.2.

Pour finir, voici le lien vers le projet Wandb pour cette section [ici](https://wandb.ai/IFT6758-2024-A05/xgboost_feature_selection).

## Expérimentation de modèles

### Préparation des données pour les modèles

Nous avons mis en commun la préparation des données pour les modèles avancés.

Pour cela, nous avons développé une fonction `load_train_val_test_x_y` qui charge les saisons 2016 à 2020 en DataFrame
puis enrichit les features (fichier `ift6758/features/load_data.py`).
Les saisons 2016 à 2019 sont utilisées pour l'entraînement et la saison 2020 pour le test.
De là, une autre séparation est effectuée pour obtenir les données d'entraînement et de validation.
Au final, nous avons 6 Dataframes :

- `X_train` : les données d'entraînement. 80% des événements des saisons 2016 à 2019.
- `y_train` : les cibles d'entraînement. Colonne `is_goal`.
- `X_val` : les données de validation. 20% des événements des saisons 2016 à 2019.
- `y_val` : les cibles de validation.
- `X_test` : les données de test. Saison 2020 sans la colonne `is_goal`.
- `y_test` : les cibles de test.

Afin de n'inclure aucune donnée relative aux événements `goal`, ce qui conduirait à un overfitting,
nous avons retiré un grand nombre de colonnes des DataFrames `X_train`, `X_val` et `X_test`.

Cette fonction peut être utilisée comme ceci :

```python
from ift6758.features import load_train_val_test_x_y
X_train, y_train, X_val, y_val, X_test, y_test = load_train_val_test_x_y(test_size=0.2)
```

#### Pipeline Scikit-learn

Nous avons créé un pipeline Scikit-learn pour préparer les données avant de les envoyer dans les modèles (fichier `ift6758/features/preprocessing_pipeline.py`).
Cette deuxième préparation consiste à encoder les variables catégorielles, imputer les valeurs manquantes et normaliser les données.
En respectant l'interface Scikit-learn, ce pipeline peut être utilisé dans un autre pipeline Scikit-learn.
Par exemple :

```python
from ift6758.features import get_preprocessing_pipeline
from sklearn.pipeline import make_pipeline

preprocessing_pipeline = get_preprocessing_pipeline()
pipeline = make_pipeline(preprocessing_pipeline, model)
```

#### Model trainer

Enfin, nous avons créé une fonction `train_and_val_model` afin d'abstraite la procédure d'entraînement, de validation 
et de sauvegarde des résultats dans Weights & Biases (fichier `advanced_models/model_trainer.py`).
Cette méthode prend en argument le modèle à entraîner (incluant le pipeline de preprocessing) puis quelques
méta-informations sur l'expérience.

Ceci nous permet de ne pas répéter le code d'entraînement et de validation pour chaque modèle.

Le chargement des données fournit par la fonction `load_train_val_test_x_y` est relativement long (environ quatre minutes).
Ce qui est normal compte tenu de la quantité de données à traiter pour obtenir les DataFrames de la partie 
**Ingénierie des caractéristiques II**.

Pour remédier à ce problème, la fonction `train_and_val_model` garde en cache les DataFrames dans des fichiers `.pkl`
pour les recharger plus rapidement entre chaque entraînement de modèle.

### Question 1 :

#### Random Forest

Nous avons entraîné un modèle Random Forest (fichier `advanced_models/random_forest.py`) et effectué une recherche par
grille pour optimiser les hyper-paramètres. Les résultats se trouvent dans le projet `random_forest` sur Weights & Biases.

Le meilleur modèle a été entraîné avec les hyper-paramètres suivants :

- `n_estimators` : 1000
- `max_depth` : 18

Son score AUC sur les données de validation est de **0.74252**.

<img class="figure" src="/images/models/random_forest/calibration_curve_random_forest.png" />
<img class="figure" src="/images/models/random_forest/cumul_goal_prop_vs_proba_random_forest.png" />
<img class="figure" src="/images/models/random_forest/goal_rate_vs_proba_random_forest.png" />
<img class="figure" src="/images/models/random_forest/roc_random_forest.png" />

#### SVM

Nous avons également entraîné un modèle SVM (fichier `advanced_models/svm.py`) et effectué une recherche par grille pour
optimiser les hyper-paramètres. Les résultats se trouvent dans le projet `svm` sur Weights & Biases.

Les résultats de ce modèle sont très médiocres, avec un score AUC sur les données de validation de **0.57** au mieux.

Il n'est pas la peine de présenter les figures pour ce modèle.

#### MLP

De plus, nous avons entraîné un modèle MLPClassifier (fichier `advanced_models/neural_network.py`) avec ses paramètres par défaut. Il a obtenu un score AUC de **0.71** sur les données de validation.

#### EN BREF

Le meilleur modèle trouvé dans cette section est le modèle Random Forest. D'ailleurs c'est lui que nous avons utilisé sur l'ensemble de test.

### Question 2 :

Voici le lien vers l'expérience du [Random Forest](https://wandb.ai/IFT6758-2024-A05/random_forest).

Voici le lien vers l'expérience du [SVM](https://wandb.ai/IFT6758-2024-A05/svm).

Voici le lien vers l'expérience du [MLP](https://wandb.ai/IFT6758-2024-A05/ift6758-milestone-2/runs/ejn3hp93).

## Évaluation sur l'ensemble de test

### Question 1 :

L'objectif principal est de déterminer si les modèles généralisent bien à l'ensemble de test et d'examiner les forces et faiblesses relatives des différents algorithmes pour les matchs de saison régulière.

Commençons par le Random Forest, qui se distingue dans toutes les visualisations. Avec une AUC de 0,71 dans la courbe ROC, il montre une capacité supérieure à différencier les classes positives et négatives, surpassant tous les modèles de régression logistique. Dans le graphique montrant le taux d'événements positifs (goal rate) par centiles de probabilité prédite, le Random Forest atteint des taux élevés dans les centiles supérieurs, indiquant que ses probabilités sont bien alignées avec les taux réels. Pour le graphique des pourcentages cumulatifs de goals, il capture rapidement une proportion importante des événements positifs dans les premiers centiles, démontrant son efficacité pour prioriser les cas les plus probables. Enfin, la courbe de calibration montre que le Random Forest est bien calibré dans l’ensemble, bien qu’il ait tendance à légèrement sur-estimer les probabilités dans les intervalles supérieurs. En comparaison avec les performances sur les données de validation, le Random Forest montre une cohérence remarquable. L’AUC sur l’ensemble de test est légèrement supérieure (0,74) par rapport à l’ensemble de validation (0,72), ce qui suggère que le modèle non seulement généralise bien. En général, le modèle reste globalement bien calibré, mais des ajustements pourraient être nécessaires.

Les modèles de régression logistique basés sur la distance et la combinaison des deux variables ("both") montrent des performances cohérentes et proches du Random Forest pour certaines métriques. Dans la courbe ROC, ces modèles affichent une AUC de 0,68, ce qui indique une bonne capacité à différencier les classes, bien qu’un peu inférieure à celle du Random Forest. Dans le graphique des taux d’événements positifs par centiles, ces modèles montrent des performances légèrement en dessous, atteignant des taux de réussite élevés, mais pas au même niveau que le Random Forest. Concernant le graphique cumulatif des pourcentages de goals, ces modèles nécessitent davantage de centiles pour capturer une proportion équivalente d'événements positifs, confirmant leur efficacité relative mais limitée par rapport au Random Forest. Enfin, dans la courbe de calibration, ces modèles sont en general bien calibrés, avec des écarts dans les centiles intermédiaires et supérieurs, où les probabilités prédictives dévient un peu des taux réels. En comparaison avec les performances sur les données de validation,ces modèles montrent une bonne généralisation. Les valeurs d’AUC sur les ensembles de validation étaient très proches de 0,68, confirmant que les performances observées sur l’ensemble de test sont cohérentes avec celles obtenues pendant la phase de validation. Par contre, il y a une baisse de performance dans les taux d’événements positifs par centiles et dans le graphique cumulatif des pourcentages de goals.

En revanche, le modèle de régression logistique basé uniquement sur l’angle ne performe pas bien. Avec une AUC de seulement 0,51 dans la courbe ROC, il ne fait pas mieux qu’un classificateur aléatoire. On peut aussi voir que ce modèle a du mal à identifier les cas positifs, avec des taux de réussite faibles et non performante. Le graphique cumulatif des pourcentages de goals montre une courbe plate, confirmant qu'il n'est pas capable de capturer efficacement les événements positifs, même dans les centiles supérieurs. La courbe de calibration illustre également cette faiblesse. En comparaison avec les performances sur les données de validation, ce modèle montre des résultats similaires, confirmant qu’il ne généralise pas bien aux nouvelles données. Les performances indique que les limitations observées ne sont pas spécifiques à l'ensemble de test, mais sont inhérentes au modèle lui-même. Cela montre l’importance de combiner des variables significatives pour améliorer les performances globales.


### Question 2 :

